{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0) mini config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_FOLDER = 'v00_original'\n",
    "TRAIN_FILENAME = 'train.pkl'\n",
    "TEST_FILENAME = 'test.pkl'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) libs and consts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\user01\\\\Desktop\\\\sibur-production\\\\input\\\\activity-atactic'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os,sys,inspect,pickle #;sys.exit(sys.path) # sys.exit(np.array([5,52,12]))\n",
    "currentdir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "parentdir = os.path.dirname(currentdir)\n",
    "gragrandparentdir = os.path.dirname(os.path.dirname(os.path.dirname(currentdir))) \n",
    "#sys.path.insert(0,gragrandparentdir) # pipeline-template\n",
    "import time ;tic = time.time() #print(time.time()-tic,'sec')\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import config\n",
    "#import kglpipe\n",
    "#path = gragrandparentdir + kglpipe.myconfig['PATH'] + INPUT_FOLDER # path = os.path.join(gragrandparentdir, kglpipe.myconfig['PATH']+INPUT_FOLDER)\n",
    "parentdir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train (463058, 60) test (103651, 56) activity_test_target (85891, 0)\n"
     ]
    }
   ],
   "source": [
    "train_data = pickle.load( open(os.path.join(parentdir, INPUT_FOLDER, TRAIN_FILENAME), \"rb\"))\n",
    "test_data = pickle.load( open(os.path.join(parentdir, INPUT_FOLDER, TEST_FILENAME), \"rb\"))\n",
    "activity_test_target = pd.read_csv(os.path.join(parentdir, INPUT_FOLDER, \"activity_test_timestamps.csv\"), index_col=\"date\", parse_dates=[\"date\"])\n",
    "print('train',train_data.shape, 'test',test_data.shape, 'activity_test_target',activity_test_target.shape)#, 'atactic_test_target',atactic_test_target.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Timestamp('2018-02-13 08:30:00')] [Timestamp('2018-12-31 22:07:00')]\n",
      "[Timestamp('2019-01-01 00:30:00')] [Timestamp('2019-03-14 00:00:00')]\n"
     ]
    }
   ],
   "source": [
    "print(list(train_data.index)[:1],list(train_data.index)[-1:])\n",
    "print(list(test_data.index)[:1],list(test_data.index)[-1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2019-01-01 00:30:00')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.index[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) update\n",
    "    step 1 'train' -----------------------------------------------------------------\n",
    "    (463058, 60)  train           with 4 targets           (feb2018-dec2018)\n",
    "    (463058, 4)   train_targets  (activity has 308313 not nulls)\n",
    "    (103651, 56)  test           without 4 targets         (jan2019-mar2019)\n",
    "    (566709, 55)  data           without 4 targets and f28\n",
    "                   +\n",
    "    (566709, 825) all_features    has its unique 825 (55x3x5) columns, generated from data\n",
    "                   ||\n",
    "    (566709, 880) full_data = data + all_features = without 4 targets and f28\n",
    "\n",
    "    (463058, 881)  activity_train_with_nulls = train_targets[[\"activity\"]] + full_data - rows\n",
    "                               463058 x1                       566709 x880  \n",
    "\n",
    "    (300737, 881) activity_train  (without rows with any NaN) save as train.pkl\n",
    "    (103651, 880) activity_test  = full_data from jan2019 => useless! save as test_useless.pkl\n",
    "    \n",
    "    step 2 'test' -----------------------------------------------------------------\n",
    "    (85891,    0) activity_test_target (empty csv with dates: jan2019-mar2019)\n",
    "                    + (join, fillna)\n",
    "    (566709, 880) full_data (see in top)\n",
    "                    ||\n",
    "    (85891,  880) test_activity_data  => real test! save as test.pkl\n",
    "    \n",
    "    conclusion: use only activity, add 825 cols to train, delete null rows and col f28\n",
    "        * for testing - predict full_data from jan2019\n",
    "        * for trainig - split activity_data on train/dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data (566709, 56)\n",
      "train_targets (463058, 4)\n"
     ]
    }
   ],
   "source": [
    "data = pd.concat([train_data[test_data.columns], test_data]) ;print('data',data.shape) # use all columns except 4 targets\n",
    "train_targets = train_data[[\"activity\", \"atactic_1\", \"atactic_2\", \"atactic_3\"]].copy() ;print('train_targets',train_targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "308313 rows without any null in whole row\n"
     ]
    }
   ],
   "source": [
    "series = train_targets[['activity']].notnull().all(axis=1)\n",
    "print(len(series[series==True]), 'rows without any null in whole row') # 300759 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Val1</th>\n",
       "      <th>Val2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-02-24 00:00:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-02-24 01:00:00</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-02-24 02:00:00</th>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-02-24 03:00:00</th>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-02-24 04:00:00</th>\n",
       "      <td>7.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Val1  Val2\n",
       "Date                           \n",
       "2015-02-24 00:00:00   0.0   1.0\n",
       "2015-02-24 01:00:00   1.0   3.0\n",
       "2015-02-24 02:00:00   3.0   5.0\n",
       "2015-02-24 03:00:00   5.0   7.0\n",
       "2015-02-24 04:00:00   7.0   9.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "rng = pd.date_range('2015-02-24', periods=5, freq='H') # freq='T'\n",
    "df = pd.DataFrame({ 'Date': rng, 'Val1': range(len(rng)), 'Val2': range(1,len(rng)+1) })\n",
    "df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
    "df.index = df['Date']\n",
    "df = df.drop(columns=['Date'])\n",
    "#     Date                  Val1 Val2  \n",
    "#     2015-02-24 00:00:00   0     1\n",
    "#     2015-02-24 01:00:00   1     2\n",
    "#     2015-02-24 02:00:00   2     3\n",
    "#     2015-02-24 03:00:00   3     4\n",
    "#     2015-02-24 04:00:00   4     5\n",
    "df.rolling('120MIN').aggregate('sum')\n",
    "#     Date                 Val1    Val2\n",
    "#     2015-02-24 00:00:00   0.0    1.0 # has no previous, but != NaN, cause date\n",
    "#     2015-02-24 01:00:00   1.0    3.0 # currVal1 = 1; prev = 0; sum = 1\n",
    "#     2015-02-24 02:00:00   3.0    5.0 # currVal1 = 2; prev = 1; sum = 3\n",
    "#     2015-02-24 03:00:00   5.0    7.0\n",
    "#     2015-02-24 04:00:00   7.0    9.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "391970892ccb48cbb6f5e6d1ba7dd388",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55MIN mean,55MIN median,55MIN std,55MIN max,55MIN min,3H mean,3H median,3H std,3H max,3H min,6H mean,6H median,6H std,6H max,6H min,\n",
      "all_features (566709, 825)\n"
     ]
    }
   ],
   "source": [
    "# generate 825 new cols from joineddata\n",
    "#data.drop(['f28','f25','f42'], axis=1, inplace=True)\n",
    "data.drop(\"f28\", axis=1, inplace=True)\n",
    "#ACOLS = [\"atactic_1\", \"atactic_2\", \"atactic_3\"]\n",
    "#not_null_atactic = train_targets.loc[train_targets[ACOLS].notnull().all(axis=1), ACOLS] ;print(not_null_atactic.shape)\n",
    "PERIODS = [\"55MIN\",\"3H\", \"6H\"]#['1D', '4D', '25D', '45D', '155D']#[\"5H\", \"4H\", \"6H\"]#[\"1H\", \"3H\", \"6H\"]\n",
    "AGGREGATES = [\"mean\", \"median\", \"std\", \"max\", \"min\"]\n",
    "#PERIODS = [\"3H\"]\n",
    "#AGGREGATES = [\"mean\"]\n",
    "all_features = []\n",
    "for period in tqdm.tqdm_notebook(PERIODS):\n",
    "    for agg in AGGREGATES:\n",
    "        print(period,agg,end=',')\n",
    "        rolling_features = data.rolling(period).aggregate(agg)\n",
    "        rolling_features.rename(lambda x: \"_\".join([x, period, agg]), axis=1, inplace=True)\n",
    "        all_features.append(rolling_features)\n",
    "all_features = pd.concat(all_features, axis=1) \n",
    "print('all_features',all_features.shape) #825/5/3 = 55 #15 new cols for each of 55 features \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full_data (566709, 880)\n"
     ]
    }
   ],
   "source": [
    "full_data = data.join(all_features)\n",
    "#full_data = data\n",
    "print('full_data',full_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-e6c409c2298b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# add col 'activity' from train to our data (train+test joined generated). 463058, cause in train so\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mactivity_train_with_nulls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_targets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"activity\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfull_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshift\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfreq\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"H\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# add 6 hours empty rows to start, delete last 6 hours rows\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'activity_train_with_nulls'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mactivity_train_with_nulls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programs\\x64\\Anaconda_3_2019_07\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mshift\u001b[1;34m(self, periods, freq, axis, fill_value)\u001b[0m\n\u001b[0;32m   4045\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mshift\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mperiods\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfreq\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4046\u001b[0m         return super(DataFrame, self).shift(periods=periods, freq=freq,\n\u001b[1;32m-> 4047\u001b[1;33m                                             axis=axis, fill_value=fill_value)\n\u001b[0m\u001b[0;32m   4048\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4049\u001b[0m     def set_index(self, keys, drop=True, append=False, inplace=False,\n",
      "\u001b[1;32mD:\\Programs\\x64\\Anaconda_3_2019_07\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mshift\u001b[1;34m(self, periods, freq, axis, fill_value)\u001b[0m\n\u001b[0;32m   8944\u001b[0m                                         fill_value=fill_value)\n\u001b[0;32m   8945\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 8946\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtshift\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mperiods\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfreq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   8947\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   8948\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_constructor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programs\\x64\\Anaconda_3_2019_07\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mtshift\u001b[1;34m(self, periods, freq, axis)\u001b[0m\n\u001b[0;32m   9036\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   9037\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 9038\u001b[1;33m             \u001b[0mnew_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   9039\u001b[0m             \u001b[0mnew_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mblock_axis\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshift\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mperiods\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfreq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   9040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programs\\x64\\Anaconda_3_2019_07\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36mcopy\u001b[1;34m(self, deep)\u001b[0m\n\u001b[0;32m    732\u001b[0m             \u001b[0mnew_axes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    733\u001b[0m         return self.apply('copy', axes=new_axes, deep=deep,\n\u001b[1;32m--> 734\u001b[1;33m                           do_integrity_check=False)\n\u001b[0m\u001b[0;32m    735\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    736\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mas_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtranspose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitems\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programs\\x64\\Anaconda_3_2019_07\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, f, axes, filter, do_integrity_check, consolidate, **kwargs)\u001b[0m\n\u001b[0;32m    350\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    351\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mconsolidate\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 352\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_consolidate_inplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    353\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    354\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'where'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programs\\x64\\Anaconda_3_2019_07\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36m_consolidate_inplace\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    927\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_consolidate_inplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_consolidated\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 929\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblocks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_consolidate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    930\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_consolidated\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_known_consolidated\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programs\\x64\\Anaconda_3_2019_07\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36m_consolidate\u001b[1;34m(blocks)\u001b[0m\n\u001b[0;32m   1897\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0m_can_consolidate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroup_blocks\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgrouper\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1898\u001b[0m         merged_blocks = _merge_blocks(list(group_blocks), dtype=dtype,\n\u001b[1;32m-> 1899\u001b[1;33m                                       _can_consolidate=_can_consolidate)\n\u001b[0m\u001b[0;32m   1900\u001b[0m         \u001b[0mnew_blocks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_extend_blocks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmerged_blocks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_blocks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1901\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mnew_blocks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programs\\x64\\Anaconda_3_2019_07\\lib\\site-packages\\pandas\\core\\internals\\blocks.py\u001b[0m in \u001b[0;36m_merge_blocks\u001b[1;34m(blocks, dtype, _can_consolidate)\u001b[0m\n\u001b[0;32m   3147\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3148\u001b[0m         \u001b[0margsort\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_mgr_locs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3149\u001b[1;33m         \u001b[0mnew_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_values\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0margsort\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3150\u001b[0m         \u001b[0mnew_mgr_locs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_mgr_locs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0margsort\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3151\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# add col 'activity' from train to our data (train+test joined generated). 463058, cause in train so\n",
    "activity_train_with_nulls = train_targets[[\"activity\"]].join(full_data.shift(6, freq=\"H\")) # add 6 hours empty rows to start, delete last 6 hours rows\n",
    "print('activity_train_with_nulls',activity_train_with_nulls.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series = activity_train_with_nulls.notnull().all(axis=1)\n",
    "print(len(series[series==True]), 'rows without any null in whole row') # 300759 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activity_train = activity_train_with_nulls[activity_train_with_nulls.notnull().all(axis=1)]\n",
    "print('activity_train',activity_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activity_test = full_data[full_data.index >= test_data.index[0]]\n",
    "print('activity_test',activity_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_activity_data = activity_test_target.join(full_data.shift(6, freq=\"H\")).ffill() # Synonym for DataFrame.fillna() with method='ffill'.\n",
    "print('test_activity_data',test_activity_data.shape) # test_activity_data (85891, 880)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) save\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activity_train.to_pickle(os.path.join(currentdir, \"train.pkl\"))\n",
    "activity_test.to_pickle(os.path.join(currentdir, \"test_useless.pkl\"))\n",
    "test_activity_data.to_pickle(os.path.join(currentdir, \"test.pkl\"))\n",
    "\n",
    "print(time.time()-tic,'sec') # 173.91042184829712 sec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "# tic = time.time() \n",
    "# train_fun = pickle.load( open(os.path.join(currentdir, TRAIN_FILENAME+FORMAT), \"rb\"))\n",
    "# test_fun = pickle.load( open(os.path.join(currentdir, TEST_FILENAME+FORMAT), \"rb\"))\n",
    "# print('train_fun',train_fun.shape, 'test_fun',test_fun.shape)#, 'activity_test_timestamps',activity_test_timestamps.shape, 'atactic_test_timestamps',atactic_test_timestamps.shape)\n",
    "# # train_fun (300737, 881) test_fun (103651, 880)\n",
    "# print(time.time()-tic,'sec') # 27 sec\n",
    "# # Split\n",
    "# tr_data = train_fun[:\"2018-10-13\"]\n",
    "# cv_data = train_fun[\"2018-10-14\":]\n",
    "# print('tr_data', tr_data.shape, 'cv_data',cv_data.shape) # tr_data (216400, 881) cv_data (84337, 881)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
