{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data_orig (42500, 56) 2018-04-01 00:01:00 2018-05-01 00:01:00\n"
     ]
    }
   ],
   "source": [
    "USE_TEST = False\n",
    "PREDICT = True\n",
    "NORMALIZE = False\n",
    "SMOOTH_TRAIN = False\n",
    "SUBMIT_TRAINVAL = False\n",
    "\n",
    "PRETRAINED_MODEL = False\n",
    "PRETRAINED_MODEL_NAME = 'y2019_11m_13d_09h_17min_53s_962269_tf_lstm_model.h5'\n",
    "SAVE_MODEL = False\n",
    "if PRETRAINED_MODEL: \n",
    "    SAVE_MODEL = False\n",
    "    SMOOTH_TRAIN = False\n",
    "    USE_TEST = True\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os,sys,inspect,pickle,json,time,datetime,re,random; root = os.path.dirname(os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe()))))\n",
    "import utils\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()\n",
    "%matplotlib inline\n",
    "\n",
    "conf = json.load(open('config_zoo.json'))\n",
    "TARGET_ORIG = 'activity'\n",
    "DATA_DIR = 'activity-atactic'\n",
    "EXTRA_DATA1 = 'v00_original\\\\activity_test_timestamps.csv'\n",
    "random.seed(conf['rs'])\n",
    "np.random.seed(conf['rs'])\n",
    "\n",
    "# load\n",
    "train_data_orig = pickle.load(open(os.path.join(root,'input',DATA_DIR,conf['data'],'train.pkl'),\"rb\")) ;print('train_data_orig',train_data_orig.shape,train_data_orig.index[0],train_data_orig.index[-1])\n",
    "#train_data_orig['activity_copy'] = train_data_orig['activity'] # hack for debug\n",
    "# train_data_orig['activity_smooth'] = train_data_orig['activity'].copy()\n",
    "# train_data_orig['activity_smooth'] = train_data_orig['activity_smooth'].ewm(span = 3600).mean()#.plot(style = 'b', label = ' Exponential moving average')        \n",
    "#if SMOOTH_TRAIN: train_data_orig[TARGET_ORIG] = train_data_orig[TARGET_ORIG].ewm(span = 360).mean()#.plot(style = 'b', label = ' Exponential moving average')        \n",
    "\n",
    "if USE_TEST:\n",
    "    test_data_orig  = pickle.load(open(os.path.join(root,'input',DATA_DIR,conf['data'],'test.pkl'),\"rb\"))\n",
    "    extra_data1 = pd.read_csv(os.path.join(root,'input',DATA_DIR,EXTRA_DATA1), index_col=\"date\", parse_dates=[\"date\"]) ;print('test_data_orig',test_data_orig.shape, 'extra_data1',extra_data1.shape)\n",
    "else: extra_data1 = None\n",
    "#train_data_orig[TARGET_ORIG].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_data_orig[TARGET_ORIG].ewm(span = 360).mean().plot(alpha=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data (42500, 56)\n",
      "train dates= 2018-04-01 00:01:00 2018-04-23 08:08:00 val dates= 2018-04-23 08:09:00 2018-05-01 00:01:00\n",
      "x_train (31875, 55) y_train (31875, 1) x_val (10625, 55) y_val (10625, 1) x_test (0,)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-3fb6cc35dca5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     45\u001b[0m                     \u001b[0my_test_pred_n\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test_n\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mUSE_TEST\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#;print('test predicted',y_test_pred_n.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m                     \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mextra_data1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_pred_n\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val_pred_n\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test_pred_n\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconf_i\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTARGET\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNORMALIZE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mUSE_TEST\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mUSE_TEST\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubmit_trainval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mSUBMIT_TRAINVAL\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_model\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mSAVE_MODEL\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Desktop\\sibur-production\\model\\utils.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(model, train, val, test, extra_data1, y_train_pred_n, y_val_pred_n, y_test_pred_n, conf_i, mtype, TARGET, NORMALIZE, USE_TEST, submit_trainval, save_model, smooth_pred)\u001b[0m\n\u001b[0;32m     70\u001b[0m     \u001b[0mlog_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mconf_i\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcurr_time\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlog_file\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'output'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlog_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'output'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlog_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m     \u001b[0mlog_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'output'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msubmit_trainval\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m         \u001b[0my_train_pred_p2\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mTARGET\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programs\\x64\\Anaconda_3_2019_07\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mto_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, tupleize_cols, date_format, doublequote, escapechar, decimal)\u001b[0m\n\u001b[0;32m   3018\u001b[0m                                  \u001b[0mdoublequote\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdoublequote\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3019\u001b[0m                                  escapechar=escapechar, decimal=decimal)\n\u001b[1;32m-> 3020\u001b[1;33m         \u001b[0mformatter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3021\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3022\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpath_or_buf\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programs\\x64\\Anaconda_3_2019_07\\lib\\site-packages\\pandas\\io\\formats\\csvs.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    170\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mUnicodeWriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mwriter_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 172\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_save\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    173\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programs\\x64\\Anaconda_3_2019_07\\lib\\site-packages\\pandas\\io\\formats\\csvs.py\u001b[0m in \u001b[0;36m_save\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    286\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 288\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_save_chunk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart_i\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend_i\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    289\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    290\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_save_chunk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_i\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend_i\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programs\\x64\\Anaconda_3_2019_07\\lib\\site-packages\\pandas\\io\\formats\\csvs.py\u001b[0m in \u001b[0;36m_save_chunk\u001b[1;34m(self, start_i, end_i)\u001b[0m\n\u001b[0;32m    313\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    314\u001b[0m         libwriters.write_csv_rows(self.data, ix, self.nlevels,\n\u001b[1;32m--> 315\u001b[1;33m                                   self.cols, self.writer)\n\u001b[0m",
      "\u001b[1;32mpandas/_libs/writers.pyx\u001b[0m in \u001b[0;36mpandas._libs.writers.write_csv_rows\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "features_orig = train_data_orig.columns.tolist(); features_orig.remove(TARGET_ORIG)\n",
    "fenames = ['together']# + activity_copy']\n",
    "felist = [utils.together]# + ['activity_copy']]  # [weights + stars] # [['f2']] # [rashod_r1, inside_r1_triple]\n",
    "for fi,FEATURES in enumerate(felist):\n",
    "    fename = fenames[fi]\n",
    "    #FEATURES = weights + stars\n",
    "    for TARGET in [TARGET_ORIG]:\n",
    "        \n",
    "        train_data = train_data_orig[FEATURES + [TARGET]] ;print('train_data',train_data.shape)\n",
    "        if USE_TEST: test_data = test_data_orig[FEATURES] ;print('test_data',test_data.shape)\n",
    "    \n",
    "        train, val = utils.split_rows(train_data, conf) ;print('train dates=',train.index[0],train.index[-1],'val dates=',val.index[0],val.index[-1])\n",
    "        test  = test_data if USE_TEST else None\n",
    "        if SMOOTH_TRAIN: train['activity'] = train['activity'].ewm(span = 16).mean()#.plot(style = 'b', label = ' Exponential moving average')        \n",
    "\n",
    "        # split cols to numpy and norm\n",
    "        x_train, x_val, x_test, y_train, y_val = utils.split_cols_np(train, val, test, TARGET, USE_TEST) ;print('x_train',x_train.shape, 'y_train',y_train.shape, 'x_val',x_val.shape, 'y_val',y_val.shape, 'x_test',x_test.shape)\n",
    "        if NORMALIZE: x_train_n, x_val_n, x_test_n, y_train_n, y_val_n = utils.normalize_by_train(x_train, x_val, x_test, y_train, y_val, train, TARGET, USE_TEST) ;print('x_train_n',x_train_n.shape, 'y_train_n',y_train_n.shape, 'x_val_n',x_val_n.shape, 'y_val_n', y_val_n.shape)\n",
    "        else: x_train_n, x_val_n, x_test_n, y_train_n, y_val_n = x_train, x_val, x_test, y_train, y_val\n",
    "\n",
    "        for mtype in conf['models'].split(','):\n",
    "            for conf_i in utils.split_model_config(conf[mtype]):\n",
    "                model = utils.init_model(mtype, conf_i)\n",
    "                if PRETRAINED_MODEL:\n",
    "                    #curr_time = pd.read_csv('output/log_'+mtype+'.csv', index_col=0).index[-1]\n",
    "                    #model = pickle.load(open(os.path.join('output','pretrained_models',curr_time+mtype+'_model.pkl'),\"rb\"))\n",
    "                    #model = pickle.load(open(os.path.join('output','pretrained_models',PRETRAINED_MODEL_NAME),\"rb\"))\n",
    "                    import tensorflow as tf\n",
    "                    #loaded = tf.saved_model.load(os.path.join('output','pretrained_models',PRETRAINED_MODEL_NAME))\n",
    "                    #path_h5 = os.path.join('output','pretrained_models',curr_time+mtype+'_model.h5')\n",
    "                    path_h5 = os.path.join('output','pretrained_models',PRETRAINED_MODEL_NAME)\n",
    "                    loaded = tf.keras.models.load_model(path_h5)\n",
    "                    model.model = loaded\n",
    "                else:\n",
    "                    model.prepare_data(x_train_n, y_train_n, x_val_n, y_val_n)\n",
    "                    model.compile(x_train.shape)\n",
    "                    model.fit(x_train_n, y_train_n, x_val_n, y_val_n)\n",
    "                    conf_i = utils.add_log_info(conf_i, model, conf, TARGET, FEATURES, fename)\n",
    "                if PREDICT:\n",
    "#                     y_train_pred_n = model.predict(x_train_n) ;print('train predicted',y_train_pred_n.shape)\n",
    "#                     y_val_pred_n   = model.predict(x_val_n) ;print('val predicted',y_val_pred_n.shape)\n",
    "#                     y_test_pred_n  = model.predict(x_test_n) if USE_TEST else np.array([]) ;print('test predicted',y_test_pred_n.shape)\n",
    "                    y_train_pred_n = model.predict(x_train_n,train)# ;print('train predicted',y_train_pred_n.shape)\n",
    "                    y_val_pred_n   = model.predict(x_val_n,val)# ;print('val predicted',y_val_pred_n.shape)\n",
    "                    y_test_pred_n  = model.predict(x_test_n,test) if USE_TEST else np.array([]) #;print('test predicted',y_test_pred_n.shape)\n",
    "                    \n",
    "                    utils.evaluate(model, train, val, test, extra_data1, y_train_pred_n, y_val_pred_n, y_test_pred_n, conf_i, mtype, TARGET, NORMALIZE, USE_TEST=USE_TEST, submit_trainval=SUBMIT_TRAINVAL, save_model=SAVE_MODEL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle.dump( model, open( os.path.join('output','pretrained_models','model3.pkl'), \"wb\" ) )\n",
    "#import tensorflow as tf\n",
    "#tf.saved_model.save(model.model, os.path.join('output','pretrained_models','model3.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loaded = tf.saved_model.load(os.path.join('output','pretrained_models',PRETRAINED_MODEL_NAME))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.model.save('path_to_my_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plot smoothed target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PREDICT:\n",
    "    if NORMALIZE: y_train_pred, y_val_pred, y_test_pred = utils.denormalize_by_train(y_train_pred_n, y_val_pred_n, y_test_pred_n, train, TARGET, USE_TEST)\n",
    "    else: y_train_pred, y_val_pred, y_test_pred = y_train_pred_n, y_val_pred_n, y_test_pred_n\n",
    "    y_train_pred_p = pd.Series(y_train_pred.flatten(), index=train.index, name=TARGET+\"_train_pred\").sort_index()\n",
    "    y_val_pred_p = pd.Series(y_val_pred.flatten(), index=val.index, name=TARGET+\"_val_pred\").sort_index() ;print(train[\"activity\"].shape, val[\"activity\"].shape, y_train_pred_p.shape, y_val_pred_p.shape)\n",
    "    #utils.plot_predictions(\"2018-04-23 07:50:00\", \"2018-04-23 08:25:00\", train, val, y_train_pred_p, y_val_pred_p, title='Last Activity')\n",
    "    #utils.plot_predictions(\"2018-04-01 07:50:00\", \"2018-04-23 08:25:00\", train, val, y_train_pred_p, y_val_pred_p, title='Last Activity')\n",
    "    if PRETRAINED_MODEL:\n",
    "        utils.plot_predictions(\"2018-02-13 14:31:00\", \"2018-12-31 22:07:00\", train, val, y_train_pred_p, y_val_pred_p, title='Last Activity')\n",
    "        #utils.plot_predictions(\"2018-04-23 01:16:00\", \"2018-04-24 16:16:00\", train, val, y_train_pred_p, y_val_pred_p, title='Last Activity')\n",
    "\n",
    "    else:\n",
    "        utils.plot_predictions(\"2018-02-13 14:31:00\", \"2018-12-31 22:07:00\", train, val, y_train_pred_p, y_val_pred_p, title='Last Activity')\n",
    "        #utils.plot_predictions(\"2018-11-01 06:16:00\", \"2018-11-01 12:17:00\", train, val, y_train_pred_p, y_val_pred_p, title='Last Activity')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.DataFrame([y_train_pred_p,y_val_pred_p]).T\n",
    "#train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "#features = ['activity']\n",
    "curr_data = train_data\n",
    "START = \"2018-04-01 00:01:00\"\n",
    "STOP = \"2018-05-01 00:01:00\"\n",
    "\n",
    "plt.figure(figsize=(18,12))\n",
    "sns.scatterplot(\n",
    "    data=curr_data, x=curr_data.index, \n",
    "    y='activity', \n",
    "    alpha=0.03, s=10\n",
    ")\n",
    "\n",
    "both_pred = pd.DataFrame([y_train_pred_p,y_val_pred_p]).T\n",
    "sns.scatterplot(\n",
    "    data=both_pred, x=curr_data.index, \n",
    "    y='activity_train_pred', \n",
    "    alpha=0.13, s=10\n",
    ")\n",
    "sns.scatterplot(\n",
    "    data=both_pred, x=curr_data.index, \n",
    "    y='activity_val_pred', \n",
    "    alpha=0.13, s=10\n",
    ")\n",
    "plt.axvline('2018-10-13', c='green', linewidth=1)\n",
    "plt.xlabel('Date'); plt.ylabel('Activity')\n",
    "plt.xlim(\n",
    "    curr_data.index.min() - datetime.timedelta(days=5),\n",
    "    curr_data.index.max() + datetime.timedelta(days=5));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train_pred_p.multiply(2).plot()\n",
    "# def denormalize_by_train2(y_train_pred_n, train, TARGET, USE_TEST):\n",
    "#     import numpy as np\n",
    "#     y_train_pred = y_train_pred_n * train[TARGET].std() + train[TARGET].mean()\n",
    "#     return y_train_pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plot not smoothed target (prediction is smoothed additionally)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PREDICT:\n",
    "    train_data_orig2 = pickle.load(open(os.path.join(root,'input',DATA_DIR,conf['data'],'train.pkl'),\"rb\")) ;print('train_data_orig2',train_data_orig2.shape,train_data_orig2.index[0],train_data_orig2.index[-1])\n",
    "    #train_data_orig2['activity_copy'] = train_data_orig2['activity'] # hack for debug\n",
    "\n",
    "    train_data2 = train_data_orig2[FEATURES + [TARGET]] ;print('train_data2',train_data2.shape)\n",
    "    train2, val2 = utils.split_rows(train_data2, conf) ;print('train2 dates=',train2.index[0],train2.index[-1],'val2 dates=',val2.index[0],val2.index[-1])\n",
    "\n",
    "    if NORMALIZE: y_train_pred, y_val_pred, y_test_pred = utils.denormalize_by_train(y_train_pred_n, y_val_pred_n, y_test_pred_n, train2, TARGET, USE_TEST)\n",
    "    else: y_train_pred, y_val_pred, y_test_pred = y_train_pred_n, y_val_pred_n, y_test_pred_n\n",
    "    y_train_pred_p = pd.Series(y_train_pred.flatten(), index=train2.index, name=TARGET+\"_train_pred\").sort_index()\n",
    "    y_val_pred_p = pd.Series(y_val_pred.flatten(), index=val2.index, name=TARGET+\"_val_pred\").sort_index() ;print(train2[\"activity\"].shape, val2[\"activity\"].shape, y_train_pred_p.shape, y_val_pred_p.shape)\n",
    "    #utils.plot_predictions(\"2018-04-23 07:50:00\", \"2018-04-23 08:25:00\", train, val, y_train_pred_p, y_val_pred_p, title='Last Activity')\n",
    "    #utils.plot_predictions(\"2018-04-01 07:50:00\", \"2018-04-23 08:25:00\", train2, val2, y_train_pred_p, y_val_pred_p, title='Last Activity')\n",
    "    #y_train_pred_p2 = y_train_pred_p.multiply(train2[TARGET].std()).add(train2[TARGET].mean())\n",
    "    \n",
    "    y_train_pred_p2 = y_train_pred_p\n",
    "\n",
    "    # way1 - smooth/mean\n",
    "#     y_train_pred_p2 = y_train_pred_p.ewm(span = 26).mean()\n",
    "#     y_train_pred_p2 = y_train_pred_p2.subtract(train2[TARGET].mean()).multiply(5/train2[TARGET].std()).add(train2[TARGET].mean())\n",
    "    # way2 - mean/smooth\n",
    "    y_train_pred_p2 = y_train_pred_p.subtract(train2[TARGET].mean()).multiply(5/train2[TARGET].std()).add(train2[TARGET].mean())\n",
    "    y_train_pred_p2 = y_train_pred_p2.ewm(span = 6).mean()\n",
    "    # way3 - just smooth\n",
    "#     y_train_pred_p2 = y_train_pred_p.ewm(span = 6).mean()\n",
    "    # way4 - mean of self\n",
    "#     y_train_pred_p2 = y_train_pred_p.subtract(y_train_pred_p.mean()).multiply(5/y_train_pred_p.std()).add(y_train_pred_p.mean())\n",
    "#     y_train_pred_p2 = y_train_pred_p2.ewm(span = 6).mean()\n",
    "\n",
    "    if PRETRAINED_MODEL:\n",
    "        utils.plot_predictions(\"2018-02-13 14:31:00\", \"2018-12-31 22:07:00\", train2, val2, y_train_pred_p2, y_val_pred_p, title='Last Activity')\n",
    "    else:\n",
    "        # day\n",
    "        #utils.plot_predictions(\"2018-04-22 16:50:00\", \"2018-04-23 10:01:00\", train2, val2, y_train_pred_p2, y_val_pred_p, title='Last Activity')\n",
    "        #utils.plot_predictions(\"2018-04-22 16:50:00\", \"2018-04-24 18:01:00\", train2, val2, y_train_pred_p2, y_val_pred_p, title='Last Activity')\n",
    "        # week\n",
    "        #utils.plot_predictions(\"2018-04-18 16:50:00\", \"2018-04-23 10:01:00\", train2, val2, y_train_pred_p2, y_val_pred_p, title='Last Activity')\n",
    "        # month (as smoothed)\n",
    "        #utils.plot_predictions(\"2018-04-01 07:50:00\", \"2018-04-23 08:25:00\", train2, val2, y_train_pred_p2, y_val_pred_p, title='Last Activity')\n",
    "        # month big\n",
    "        #utils.plot_predictions(\"2018-04-01 07:50:00\", \"2018-05-01 00:01:00\", train2, val2, y_train_pred_p2, y_val_pred_p, title='Last Activity')\n",
    "        # year\n",
    "        #utils.plot_predictions(\"2018-04-01 07:50:00\", \"2018-12-23 08:25:00\", train2, val2, y_train_pred_p2, y_val_pred_p, title='Last Activity')\n",
    "        #utils.plot_predictions(\"2018-11-01 06:16:00\", \"2018-11-01 12:17:00\", train2, val2, y_train_pred_p2, y_val_pred_p, title='Last Activity')\n",
    "        utils.plot_predictions(\"2018-02-13 14:31:00\", \"2018-12-31 22:07:00\", train2, val2, y_train_pred_p2, y_val_pred_p, title='Last Activity')\n",
    "\n",
    "\n",
    "    print(train2[TARGET].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model.conf['name'] == 'tf_lstm' and model.conf['step']==1: # train dates = 2018-04-01 00:01:00 2018-04-23 08:08:00\n",
    "    #model.plot_1_prediction(\"2018-04-23 08:07:00\", train, TARGET_ORIG, USE_TEST, NORMALIZE, train, felist, filler=30) # date=\"2018-04-01 00:06:00\"\n",
    "    #model.plot_1_prediction(\"2018-04-23 08:25:00\", val, TARGET_ORIG, USE_TEST, NORMALIZE, train, felist, filler=30) # date=\"2018-04-01 00:06:00\"\n",
    "    model.plot_1_prediction(\"2018-04-23 07:00:00\", train, TARGET_ORIG, USE_TEST, NORMALIZE, train, felist, filler=30) # date=\"2018-04-01 00:06:00\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USE_TEST_BIG = True\n",
    "\n",
    "# train_data_orig_big = pickle.load(open(os.path.join(root,'input',DATA_DIR,'v06_55min','train.pkl'),\"rb\")) ;print('train_data_orig_big',train_data_orig_big.shape,train_data_orig_big.index[0],train_data_orig_big.index[-1])\n",
    "# if USE_TEST_BIG:\n",
    "#     test_data_orig_big  = pickle.load(open(os.path.join(root,'input',DATA_DIR,'v06_55min','test.pkl'),\"rb\"))\n",
    "#     extra_data1 = pd.read_csv(os.path.join(root,'input',DATA_DIR,EXTRA_DATA1), index_col=\"date\", parse_dates=[\"date\"]) ;print('test_data_orig_big',test_data_orig_big.shape, 'extra_data1',extra_data1.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data = train_data_orig[FEATURES + [TARGET]] ;print('train_data',train_data.shape)\n",
    "# if USE_TEST: test_data = test_data_orig[FEATURES] ;print('test_data',test_data.shape)\n",
    "\n",
    "# train_big, val_big = utils.split_rows(train_data, conf) ;print('train dates=',train.index[0],train.index[-1],'val dates=',val.index[0],val.index[-1])\n",
    "# test_big  = test_data if USE_TEST else None\n",
    "# #if SMOOTH_TRAIN: train['activity'] = train['activity'].copy().ewm(span = 3600).mean()#.plot(style = 'b', label = ' Exponential moving average')        \n",
    "\n",
    "# # split cols to numpy and norm\n",
    "# x_train, x_val, x_test, y_train, y_val = utils.split_cols_np(train, val, test, TARGET, USE_TEST) ;print('x_train',x_train.shape, 'y_train',y_train.shape, 'x_val',x_val.shape, 'y_val',y_val.shape, 'x_test',x_test.shape)\n",
    "# if NORMALIZE: x_train_n, x_val_n, x_test_n, y_train_n, y_val_n = utils.normalize_by_train(x_train, x_val, x_test, y_train, y_val, train, TARGET, USE_TEST) ;print('x_train_n',x_train_n.shape, 'y_train_n',y_train_n.shape, 'x_val_n',x_val_n.shape, 'y_val_n', y_val_n.shape)\n",
    "# else: x_train_n, x_val_n, x_test_n, y_train_n, y_val_n = x_train, x_val, x_test, y_train, y_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
