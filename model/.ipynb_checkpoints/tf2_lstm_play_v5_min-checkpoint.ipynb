{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature groups\n",
    "rashod_r1 = ['f2','f3','f4','f5','f20','f22']\n",
    "inside_r1_triple = ['f17','f25','f29','f32','f46','f47','f41']\n",
    "inside_r1_double = ['f7','f8','f9']\n",
    "inside_r1_unique = ['f39']\n",
    "\n",
    "rashod_r2 = ['f10','f11','f12','f13','f21','f23']\n",
    "inside_r2_triple = ['f18','f26','f31','f34','f51','f52','f50']\n",
    "inside_r2_double = ['f14','f15','f16']\n",
    "\n",
    "inside_r3_triple = ['f19','f27','f30','f33','f48','f49','f45']\n",
    "inside_r3_unique = ['f36','f40']\n",
    "\n",
    "\n",
    "weights = ['f53','f54','f55']\n",
    "weather = ['f42','f43','f44']\n",
    "temps_pro_wat_d219 = ['f1','f24','f38']\n",
    "pressures_pro_az_etil = ['f6','f35','f37']\n",
    "donor_in_nefras_level = ['f0']\n",
    "bad = ['f28']\n",
    "stars = ['f17','f18','f39','f50','f41','f38','f40','f45','f19'] # pred from activity gives e-2\n",
    "rects = ['f52','f51','f46','f47','f48','f49','f36'] # pred from activity gives e-1\n",
    "\n",
    "# groups of groups\n",
    "rashod_r1_r2 = rashod_r1 + rashod_r2\n",
    "inside_r1_r2 = inside_r1_triple + inside_r1_double + inside_r1_unique + inside_r2_triple + inside_r2_double\n",
    "inside_r3 = inside_r3_triple + inside_r3_unique\n",
    "unknown = weights + weather + temps_pro_wat_d219 + pressures_pro_az_etil + donor_in_nefras_level\n",
    "together = rashod_r1_r2 + inside_r1_r2 + inside_r3 + unknown\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300737, 331)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os,sys,inspect,pickle,json,time,datetime,re; root = os.path.dirname(os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe()))))\n",
    "import random as rn\n",
    "import utils\n",
    "%matplotlib inline\n",
    "\n",
    "conf = json.load(open('config_zoo.json'))\n",
    "TARGET_ORIG = 'activity'\n",
    "DATA_DIR = 'activity-atactic'\n",
    "EXTRA_DATA1 = 'v00_original\\\\activity_test_timestamps.csv'\n",
    "rn.seed(conf['rs'])\n",
    "np.random.seed(conf['rs'])\n",
    "\n",
    "# load\n",
    "train_data_orig = pickle.load(open(os.path.join(root,'input',DATA_DIR,conf['data'],'train.pkl'),\"rb\"))\n",
    "test_data_orig  = pickle.load(open(os.path.join(root,'input',DATA_DIR,conf['data'],'test.pkl'),\"rb\"))\n",
    "features_orig = train_data_orig.columns.tolist(); features_orig.remove(TARGET_ORIG)\n",
    "\n",
    "#train_data_orig = train_data_orig[:\"2018-04-13\"]\n",
    "train_data_orig.shape # (300737, 331)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SPLIT = 250000#40000#200000#300#300#300000\n",
    "BUFFER_SIZE = 10000#100#10000#100#10000\n",
    "BATCH_SIZE = 256#1024#32#128#256##256\n",
    "EVALUATION_INTERVAL = 50#2#200\n",
    "EPOCHS = 3\n",
    "\n",
    "past_history = 360#172#0#17200#0#720\n",
    "future_target = 360#7#72\n",
    "STEP = 1#100#10#6#6\n",
    "\n",
    "features_considered = [TARGET_ORIG] + ['f53']#rashod_r1_r2#train_data_orig.columns.tolist()#['col1', 'col2', 'col3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data_orig (300737, 331) features (300737, 13) dataset (300737, 13)\n"
     ]
    }
   ],
   "source": [
    "#from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import tensorflow as tf\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import utils_lstm as utils\n",
    "mpl.rcParams['figure.figsize'] = (8, 6)\n",
    "mpl.rcParams['axes.grid'] = False\n",
    "tf.random.set_seed(13)\n",
    "\n",
    "# dates = pd.to_datetime(pd.date_range('2018-02-13', periods=400, freq='MIN', name='date'), errors='coerce') # freq='T'\n",
    "# col1 = np.arange(len(dates)) * 1 # np.random.randn(len(dates))\n",
    "# col2 = np.array([np.sin(i/10) for i in range(1,(len(dates)+1))]) * 2 # np.random.randn(len(dates))\n",
    "# col3 = np.array([np.sin(i/50) for i in range(1,(len(dates)+1))]) * 3 # np.random.randn(len(dates))\n",
    "# train_data_orig = pd.DataFrame({'col1':col1,'col2':col2,'col3':col3}, index=dates)\n",
    "\n",
    "features = train_data_orig[features_considered]\n",
    "features.index = train_data_orig.index\n",
    "dataset = features.values\n",
    "\n",
    "print('train_data_orig',train_data_orig.shape, 'features',features.shape, 'dataset',dataset.shape)\n",
    "#features.plot(subplots=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>activity</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "      <th>f4</th>\n",
       "      <th>f5</th>\n",
       "      <th>f20</th>\n",
       "      <th>f22</th>\n",
       "      <th>f10</th>\n",
       "      <th>f11</th>\n",
       "      <th>f12</th>\n",
       "      <th>f13</th>\n",
       "      <th>f21</th>\n",
       "      <th>f23</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-02-13 14:31:00</th>\n",
       "      <td>26.492102</td>\n",
       "      <td>1.675090</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.024659</td>\n",
       "      <td>0.001633</td>\n",
       "      <td>1.573941</td>\n",
       "      <td>0.009769</td>\n",
       "      <td>1.681415</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>0.024994</td>\n",
       "      <td>0.001625</td>\n",
       "      <td>1.612820</td>\n",
       "      <td>0.009681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-13 14:32:00</th>\n",
       "      <td>34.102710</td>\n",
       "      <td>1.675766</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.024727</td>\n",
       "      <td>0.001639</td>\n",
       "      <td>1.573461</td>\n",
       "      <td>0.009792</td>\n",
       "      <td>1.681492</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>0.024928</td>\n",
       "      <td>0.001629</td>\n",
       "      <td>1.614684</td>\n",
       "      <td>0.009695</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      activity        f2        f3        f4        f5  \\\n",
       "date                                                                     \n",
       "2018-02-13 14:31:00  26.492102  1.675090  0.000051  0.024659  0.001633   \n",
       "2018-02-13 14:32:00  34.102710  1.675766  0.000051  0.024727  0.001639   \n",
       "\n",
       "                          f20       f22       f10      f11       f12  \\\n",
       "date                                                                   \n",
       "2018-02-13 14:31:00  1.573941  0.009769  1.681415  0.00005  0.024994   \n",
       "2018-02-13 14:32:00  1.573461  0.009792  1.681492  0.00005  0.024928   \n",
       "\n",
       "                          f13       f21       f23  \n",
       "date                                               \n",
       "2018-02-13 14:31:00  0.001625  1.612820  0.009681  \n",
       "2018-02-13 14:32:00  0.001629  1.614684  0.009695  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([26.49210181, 34.10271023, 41.51822187, ..., 45.38970385,\n",
       "       45.54252189, 45.06096939])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# norm\n",
    "data_mean = dataset[:TRAIN_SPLIT].mean(axis=0)\n",
    "data_std = dataset[:TRAIN_SPLIT].std(axis=0)\n",
    "dataset = (dataset-data_mean)/data_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-9591a28e95e8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m x_train_multi, y_train_multi = utils.multivariate_data(dataset, dataset[:, 0], 0,\n\u001b[0;32m      3\u001b[0m                                                  \u001b[0mTRAIN_SPLIT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpast_history\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m                                                  future_target, STEP)\n\u001b[0m\u001b[0;32m      5\u001b[0m x_val_multi, y_val_multi = utils.multivariate_data(dataset, dataset[:, 0],\n\u001b[0;32m      6\u001b[0m                                              \u001b[0mTRAIN_SPLIT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpast_history\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\sibur-production\\model\\utils_lstm.py\u001b[0m in \u001b[0;36mmultivariate_data\u001b[1;34m(dataset, target, start_index, end_index, history_size, target_size, step, single_step)\u001b[0m\n\u001b[0;32m     54\u001b[0m       \u001b[0mlabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mtarget_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmulti_step_plot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrue_future\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprediction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSTEP\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# split rows and cols\n",
    "x_train_multi, y_train_multi = utils.multivariate_data(dataset, dataset[:, 0], 0,\n",
    "                                                 TRAIN_SPLIT, past_history,\n",
    "                                                 future_target, STEP)\n",
    "x_val_multi, y_val_multi = utils.multivariate_data(dataset, dataset[:, 0],\n",
    "                                             TRAIN_SPLIT, None, past_history,\n",
    "                                             future_target, STEP)\n",
    "print('x_train_multi',x_train_multi.shape, 'y_train_multi',y_train_multi.shape)\n",
    "print('x_val_multi',x_val_multi.shape, 'y_val_multi',y_val_multi.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join cols (tf input format)\n",
    "train_data_multi = tf.data.Dataset.from_tensor_slices((x_train_multi, y_train_multi))\n",
    "train_data_multi = train_data_multi.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n",
    "\n",
    "val_data_multi = tf.data.Dataset.from_tensor_slices((x_val_multi, y_val_multi))\n",
    "val_data_multi = val_data_multi.batch(BATCH_SIZE).repeat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile\n",
    "multi_step_model = tf.keras.models.Sequential()\n",
    "multi_step_model.add(tf.keras.layers.LSTM(32,\n",
    "                                          return_sequences=True,\n",
    "                                          input_shape=x_train_multi.shape[-2:]))\n",
    "multi_step_model.add(tf.keras.layers.LSTM(16, activation='relu'))\n",
    "multi_step_model.add(tf.keras.layers.Dense(future_target))#(72))\n",
    "\n",
    "multi_step_model.compile(optimizer=tf.keras.optimizers.RMSprop(clipvalue=1.0), loss='mae')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit\n",
    "multi_step_history = multi_step_model.fit(train_data_multi, epochs=EPOCHS,\n",
    "                                          steps_per_epoch=EVALUATION_INTERVAL,\n",
    "                                          validation_data=val_data_multi,\n",
    "                                          validation_steps=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plot_train_history(multi_step_history, 'Multi-Step Training and validation loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in val_data_multi.take(3):\n",
    "  utils.multi_step_plot(x[0], y[0], multi_step_model.predict(x)[0], STEP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
